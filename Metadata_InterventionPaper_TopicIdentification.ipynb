{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "import json\n",
    "import re\n",
    "import gensim\n",
    "from sklearn.decomposition import LatentDirichletAllocation as LDA\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/lordh1/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/lordh1/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import file\n",
    "- Only keep rows with non-null titles and abstracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = pd.read_csv('metadata 5.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = m[(m['title'].notna() & m['abstract'].notna())]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatize Abstracts and Titles (get word roots)\n",
    "\n",
    "- lowercase\n",
    "- remove punctuation\n",
    "- remove stopwords\n",
    "- get root of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_tokenizer = WhitespaceTokenizer()\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(sentence):\n",
    "    sentence = sentence.lower()\n",
    "    sentence_no_punctuation = sentence.translate(str.maketrans('', '', string.punctuation))\n",
    "    lemmatized_list = [lemmatizer.lemmatize(w) for w in w_tokenizer.tokenize(sentence_no_punctuation) \n",
    "                  if w not in stopwords.words('english')]\n",
    "    return lemmatized_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "m['abstract_lemmatized']=m['abstract'].map(lambda s:preprocess(s)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_words = list(m['abstract_lemmatized'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) # higher threshold fewer phrases.\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigram = gensim.models.Phrases(bigram[data_words], threshold=100)  \n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_bigrams(texts):\n",
    "    return [bigram_mod[doc] for doc in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_trigrams(texts):\n",
    "    return [trigram_mod[bigram_mod[doc]] for doc in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "m['abstract_lemmatized_grams']= make_trigrams(m['abstract_lemmatized'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "def abstract_to_string(text):\n",
    "    return ' '.join(word for word in text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "m['cleanAbstract'] = m['abstract_lemmatized_grams'].map(lambda s:abstract_to_string(s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA on abstracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer = CountVectorizer(stop_words='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_vectorized = count_vectorizer.fit_transform(m['cleanAbstract'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Completed GridSearch for  n_components (options 5, 10, 15, 20, 25, 30, 35). The best parameter was 5 - used below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_topics = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = LDA(n_components=number_topics, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LatentDirichletAllocation(batch_size=128, doc_topic_prior=None,\n",
       "                          evaluate_every=-1, learning_decay=0.7,\n",
       "                          learning_method='batch', learning_offset=10.0,\n",
       "                          max_doc_update_iter=100, max_iter=10,\n",
       "                          mean_change_tol=0.001, n_components=5, n_jobs=-1,\n",
       "                          perp_tol=0.1, random_state=None,\n",
       "                          topic_word_prior=None, total_samples=1000000.0,\n",
       "                          verbose=0)"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda.fit(data_vectorized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print top words associated with Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function\n",
    "def print_topics(model, count_vectorizer, n_top_words=10):\n",
    "    words = count_vectorizer.get_feature_names()\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"\\nTopic #%d:\" % topic_idx)\n",
    "        print(\" \".join([words[i]\n",
    "                        for i in topic.argsort()[:-n_top_words - 1:-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topics found via LDA:\n",
      "\n",
      "Topic #0:\n",
      "cell virus infection viral protein response expression replication host mouse\n",
      "\n",
      "Topic #1:\n",
      "patient study disease treatment infection clinical group result case associated\n",
      "\n",
      "Topic #2:\n",
      "disease health outbreak case data infection infectious model study epidemic\n",
      "\n",
      "Topic #3:\n",
      "virus infection viral influenza respiratory sample study strain detected result\n",
      "\n",
      "Topic #4:\n",
      "protein sequence gene structure binding peptide genome region coronavirus analysis\n"
     ]
    }
   ],
   "source": [
    "# Print the topics found by the LDA model\n",
    "print(\"Topics found via LDA:\")\n",
    "print_topics(lda, count_vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "topics = lda.transform(data_vectorized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in range(number_topics):\n",
    "    col_name = 'Topic ' + str(idx)\n",
    "    m[col_name] = topics[:, idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looking for Topic associated with NPI (non-pharm in abstract)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_pharm = m[(m['abstract'].str.contains('non-pharm'))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_cols = [x for x in m.columns if 'Topic ' in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "non_pharm_topics = non_pharm[topic_cols].idxmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_frequent(List): \n",
    "    return max(set(List), key = List.count) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Take the topics that match most for NPI modeling papers, then find all papers with that as their top topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'Topic 2': 53, 'Topic 1': 4})"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(non_pharm_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Topic 2'"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_topic = most_frequent(list(non_pharm_topics))\n",
    "\n",
    "top_topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "m['Top_Topic'] = m[topic_cols].idxmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Top_Topic\n",
       "Topic 0    11938\n",
       "Topic 1     5151\n",
       "Topic 2    11364\n",
       "Topic 3     7928\n",
       "Topic 4     5822\n",
       "dtype: int64"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.groupby('Top_Topic').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "top_topic_papers = m[m['Top_Topic'] == top_topic]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keywords\n",
    "\n",
    "- Need a core covid keyword\n",
    "- And need a topic keyword"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keywords chosen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "covid_keywords = ['corona', 'covid']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "intervention_keywords = ['social distancing',\n",
    "                        'contact tracing',\n",
    "                        'case isolation',\n",
    "                        'shelter-in-place',\n",
    "                        'stay-at-home',\n",
    "                        'movement restriction',\n",
    "                        'event cancel',\n",
    "                        'face mask',\n",
    "                        'facial mask',\n",
    "                        'travel ban',\n",
    "                        'school closure']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identify core papers - about COVID-19 (keyword search and published date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_filter = '2019-12-01'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Identified 650 papers with \"corona\" in title, 1766 relevant papers with \"corona\" in abstract\n",
      "Identified 5 papers with \"covid\" in title, 37 relevant papers with \"covid\" in abstract\n"
     ]
    }
   ],
   "source": [
    "find_papers_w_keywords(covid_keywords, top_topic_papers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "top_topic_papers['core_abstract'] = top_topic_papers['abstract'].apply(lambda x: any([k in x for k in covid_keywords]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "covid_papers = top_topic_papers[(top_topic_papers['core_abstract'] == True) & \n",
    "                                (top_topic_papers['publish_time'] >= date_filter)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11364, 1129)"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(top_topic_papers), len(covid_papers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intervention Papers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "for keyword in intervention_keywords:\n",
    "    covid_papers[keyword] = covid_papers['abstract'].str.contains(keyword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "covid_papers['# Keywords in Abstract'] = covid_papers[intervention_keywords].sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_papers_w_keywords(topic_keywords, papers):\n",
    "    for keyword in topic_keywords:\n",
    "        num_papers_title = len(papers[(papers['title'].str.contains(keyword)) & \n",
    "                                        (papers['title'])])\n",
    "        num_papers_abstract = len(papers[papers['abstract'].str.contains(keyword)])\n",
    "        print ('Identified {} papers with \"{}\" in title, {} relevant papers with \"{}\" in abstract'\\\n",
    "                       .format(num_papers_title, keyword, num_papers_abstract, keyword)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Identified 3 papers with \"social distancing\" in title, 38 relevant papers with \"social distancing\" in abstract\n",
      "Identified 1 papers with \"contact tracing\" in title, 16 relevant papers with \"contact tracing\" in abstract\n",
      "Identified 0 papers with \"case isolation\" in title, 5 relevant papers with \"case isolation\" in abstract\n",
      "Identified 0 papers with \"shelter-in-place\" in title, 0 relevant papers with \"shelter-in-place\" in abstract\n",
      "Identified 0 papers with \"stay-at-home\" in title, 1 relevant papers with \"stay-at-home\" in abstract\n",
      "Identified 0 papers with \"movement restriction\" in title, 3 relevant papers with \"movement restriction\" in abstract\n",
      "Identified 0 papers with \"event cancel\" in title, 0 relevant papers with \"event cancel\" in abstract\n",
      "Identified 0 papers with \"face mask\" in title, 4 relevant papers with \"face mask\" in abstract\n",
      "Identified 0 papers with \"facial mask\" in title, 2 relevant papers with \"facial mask\" in abstract\n",
      "Identified 0 papers with \"travel ban\" in title, 9 relevant papers with \"travel ban\" in abstract\n",
      "Identified 1 papers with \"school closure\" in title, 8 relevant papers with \"school closure\" in abstract\n"
     ]
    }
   ],
   "source": [
    "find_papers_w_keywords(intervention_keywords, covid_papers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "intervention_papers = covid_papers[covid_papers['# Keywords in Abstract'] > 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(intervention_papers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Interventions to mitigate early spread of SARS-CoV-2 in Singapore: a modelling study',\n",
       " 'School closure and management practices during coronavirus outbreaks including COVID-19: a rapid systematic review',\n",
       " 'Impact of school closures for COVID-19 on the US health-care workforce and net mortality: a modelling study',\n",
       " 'Sentinel Event Surveillance to Estimate Total SARS-CoV-2 Infections, United States',\n",
       " 'Age profile of susceptibility, mixing, and social distancing shape the dynamics of the novel coronavirus disease 2019 outbreak in China',\n",
       " 'The Effectiveness of Social Distancing in Mitigating COVID-19 Spread: a modelling analysis',\n",
       " 'A Social Network Model of the COVID-19 Pandemic',\n",
       " 'A Genomic Survey of SARS-CoV-2 Reveals Multiple Introductions into Northern California without a Predominant Lineage',\n",
       " 'Pandemic Politics: Timing State-Level Social Distancing Responses to COVID-19',\n",
       " 'Will novel virus go pandemic or be contained?',\n",
       " 'Estimating Risk for Death from 2019 Novel Coronavirus Disease, China, January-February 2020',\n",
       " 'Feasibility of controlling COVID-19 outbreaks by isolation of cases and contacts']"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(intervention_papers['title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "# intervention_papers.to_csv(\"intervention_papers.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search full text files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_keyword(keywords, text):\n",
    "    \"\"\"\n",
    "    Iterates through a list of keywords and searches them in a string of text.\n",
    "\n",
    "    inputs:\n",
    "      keywords: list of keywords\n",
    "      text: string of text\n",
    "\n",
    "    output: number of times keywords are found in the text\n",
    "    \"\"\"\n",
    "    find = []\n",
    "    for keyword in keywords:\n",
    "        find.extend(re.findall(keyword, text.lower()))\n",
    "    return len(find)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_body_text(sha, folder1, folder2, keywords, sentence_only):\n",
    "    \"\"\"\n",
    "    Searches a single full length text for sentences/paragraphs which contain a list of keywords.\n",
    "\n",
    "    inputs:\n",
    "      sha: sha file name\n",
    "      folder1: text folder name\n",
    "      folder2: pdf or pmc folder name\n",
    "      keywords: list of keywords to search for\n",
    "      sentence_only: whether or not to show sentence only or full paragraph\n",
    "    \n",
    "    output: list of sentences/paragraphs found containing keywords\n",
    "    \"\"\"\n",
    "\n",
    "    #open text file\n",
    "    with open('./CORD-19-research-challenge/'+folder1+'/'+folder1+'/'+folder2+'/'+sha+'.json') as f:\n",
    "        file = json.load(f)\n",
    "    \n",
    "    found = []\n",
    "    for text_dict in file[\"body_text\"]:\n",
    "        \n",
    "        #if show_sentence_only, then split the paragraph into sentences, then look for keywords\n",
    "        if sentence_only:\n",
    "            sentences = text_dict[\"text\"].split(\". \")\n",
    "            for sentence in sentences:\n",
    "                count = find_keyword(keywords, sentence)\n",
    "                if count > 0:\n",
    "                    found.append(sentence)\n",
    "                    \n",
    "        #otherwise, show the whole paragraph\n",
    "        else:\n",
    "            count = find_keyword(keywords, text_dict[\"text\"])\n",
    "            if count > 0:\n",
    "                #print(text_dict[\"section\"])\n",
    "                found.append(text_dict[\"text\"])\n",
    "                \n",
    "    return(found)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "def automated_lit_search(metadata_subset, keywords, sentence_only=True):\n",
    "    \"\"\"\n",
    "    Creates a table keyword findings.\n",
    "    \n",
    "    inputs:\n",
    "      metadata_subset: subset of metadata file to search\n",
    "      keywords: list of keywords to search\n",
    "      sentence_only: whether or not to show sentence only or full paragraph\n",
    "    \n",
    "    output: dataframe table of results with columns containing index, title, and text snippet\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    indices = metadata_subset[metadata_subset['has_pdf_parse'] == True].index\n",
    "    indices_pmc = metadata_subset[metadata_subset['has_pmc_xml_parse'] == True].index\n",
    "    indices.append(indices_pmc)\n",
    "    \n",
    "    for index in indices:\n",
    "        \n",
    "        #find text location\n",
    "        sha = metadata_subset[\"sha\"][index].split(';')[0]\n",
    "        folder1 = metadata_subset[\"full_text_file\"][index]\n",
    "        if metadata_subset['has_pdf_parse'][index] == True:\n",
    "            folder2 = 'pdf_json'\n",
    "        elif metadata_subset['has_pmc_xml_parse'][index] == True:\n",
    "            folder2 = 'pmc_json'\n",
    "        \n",
    "        #open text and search for keywords\n",
    "        found = search_body_text(sha, folder1, folder2, keywords, sentence_only)\n",
    "        if len(found) > 0:\n",
    "            for f in found:\n",
    "                results.append([index, metadata_subset[\"title\"][index], f])\n",
    "                \n",
    "    results_df = pd.DataFrame(results, columns=[\"index\",\"title\",\"text\"])\n",
    "    return(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "# intervention_sentences = automated_lit_search(intervention_papers, intervention_keywords, True)\n",
    "# intervention_sentences.to_csv('intervention_sentences_v3.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "# intervention_paragraphs = automated_lit_search(intervention_papers, intervention_keywords, False)\n",
    "# intervention_paragraphs.to_csv('intervention_paragraphs_v3.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

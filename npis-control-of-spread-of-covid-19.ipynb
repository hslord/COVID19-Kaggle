{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer \nfrom nltk.tokenize import WhitespaceTokenizer\nfrom nltk.stem import WordNetLemmatizer\nimport gensim\nfrom sklearn.decomposition import LatentDirichletAllocation as LDA\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom collections import Counter\nimport string\nimport json\nimport re\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n# import os\n# for dirname, _, filenames in os.walk('/kaggle/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":52,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- IDs: 4743095, 2077273, Marcus Dreyer, 4894652, 4833887\n- usernames: helenlord, katieymo, Marcus Dreyer, dincerti, shemrarizzo\n- Emails: lord.helen [at] gene [dot] com, katieymo [at] gmail [dot] com, dreyer [at] itprodqs-consultng [dot] com, devin.incerti [at] gmail [dot] com, shem.rizzo [at] gmail [dot] com\n\n\n## Goal: to identify papers which discuss specific non-pharmaceutical interventions to decrease the spread of COVID-19\n\n*Focus: Methods to control the spread in communities, barriers to compliance and how these vary among different populations*\n\n\n## Methodology: \n1) Cleaned abstracts\n\n2) Use LDA on cleaned abstracts to identify papers most relevant to NPI topics\n\n3) Use keyword search to pull out NPI papers which focus on: methods to control the spread in communities, barriers to compliance and how these vary among different populations\n\n4) Pull out specific sentences and paragraphs from the identified papers with the keywords for quick identification\n\n## Notes:\n- The keyword search alone on the metadata is a useful methodology to identify relevant papers. It can be completed on either the title or the abstract. However, finding pertinent, specific keywords is very important for success\n- Cleaning the text and using topic modeling focuses on a smaller, more relevant subset of papers, allowing this methodology to scale. However, a some relevant papers may be eliminated by the topic modeling"},{"metadata":{"trusted":true},"cell_type":"code","source":"m = pd.read_csv('/kaggle/input/CORD-19-research-challenge/metadata.csv')","execution_count":53,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"m = m[(m['title'].notna() & m['abstract'].notna())]","execution_count":54,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Clean Abstracts\n\n- lowercase\n- remove punctuation\n- remove stopwords\n- lemmatize\n- bigrams/trigrams"},{"metadata":{"trusted":true},"cell_type":"code","source":"w_tokenizer = WhitespaceTokenizer()\nlemmatizer = WordNetLemmatizer()","execution_count":55,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def preprocess(sentence):\n    sentence = sentence.lower()\n    sentence_no_punctuation = sentence.translate(str.maketrans('', '', string.punctuation))\n    lemmatized_list = [lemmatizer.lemmatize(w) for w in w_tokenizer.tokenize(sentence_no_punctuation) \n                  if w not in stopwords.words('english')]\n    return lemmatized_list","execution_count":56,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"m['abstract_lemmatized']=m['abstract'].map(lambda s:preprocess(s)) ","execution_count":57,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_words = list(m['abstract_lemmatized'])","execution_count":58,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) # higher threshold fewer phrases.\nbigram_mod = gensim.models.phrases.Phraser(bigram)","execution_count":59,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trigram = gensim.models.Phrases(bigram[data_words], threshold=100)  \ntrigram_mod = gensim.models.phrases.Phraser(trigram)","execution_count":60,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def make_bigrams(texts):\n    return [bigram_mod[doc] for doc in texts]","execution_count":61,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def make_trigrams(texts):\n    return [trigram_mod[bigram_mod[doc]] for doc in texts]","execution_count":62,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"m['abstract_lemmatized_grams']= make_trigrams(m['abstract_lemmatized'])","execution_count":63,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def abstract_to_string(text):\n    return ' '.join(word for word in text)","execution_count":64,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"m['cleanAbstract'] = m['abstract_lemmatized_grams'].map(lambda s:abstract_to_string(s))","execution_count":65,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## LDA on Cleaned Abstracts"},{"metadata":{"trusted":true},"cell_type":"code","source":"count_vectorizer = CountVectorizer(stop_words='english')","execution_count":66,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_vectorized = count_vectorizer.fit_transform(m['cleanAbstract'])","execution_count":67,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Compared sklean and gensim LDA models, completed GridSearch for n_components (options 5, 10, 15, 20, 25, 30, 35). The best model was sklearn and parameter was 5 - used below"},{"metadata":{"trusted":true},"cell_type":"code","source":"number_topics = 5","execution_count":68,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lda = LDA(n_components=number_topics, n_jobs=-1)","execution_count":69,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lda.fit(data_vectorized)","execution_count":70,"outputs":[{"output_type":"execute_result","execution_count":70,"data":{"text/plain":"LatentDirichletAllocation(batch_size=128, doc_topic_prior=None,\n                          evaluate_every=-1, learning_decay=0.7,\n                          learning_method='batch', learning_offset=10.0,\n                          max_doc_update_iter=100, max_iter=10,\n                          mean_change_tol=0.001, n_components=5, n_jobs=-1,\n                          perp_tol=0.1, random_state=None,\n                          topic_word_prior=None, total_samples=1000000.0,\n                          verbose=0)"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"## Print top words associated with Topics"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Helper function\ndef print_topics(model, count_vectorizer, n_top_words=10):\n    words = count_vectorizer.get_feature_names()\n    for topic_idx, topic in enumerate(model.components_):\n        print(\"\\nTopic #%d:\" % topic_idx)\n        print(\" \".join([words[i]\n                        for i in topic.argsort()[:-n_top_words - 1:-1]]))","execution_count":71,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Print the topics found by the LDA model\nprint(\"Topics found via LDA:\")\nprint_topics(lda, count_vectorizer)","execution_count":72,"outputs":[{"output_type":"stream","text":"Topics found via LDA:\n\nTopic #0:\nprotein virus cell viral rna gene replication sequence host activity\n\nTopic #1:\nvirus influenza infection respiratory study sample viral human method case\n\nTopic #2:\ncell infection response vaccine mouse virus immune expression disease study\n\nTopic #3:\ndisease health data model outbreak infectious study case risk epidemic\n\nTopic #4:\npatient infection group virus day study clinical disease result treatment\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"topics = lda.transform(data_vectorized)","execution_count":73,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for idx in range(number_topics):\n    col_name = 'Topic ' + str(idx)\n    m[col_name] = topics[:, idx]","execution_count":74,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Looking for Topic associated with NPI (non-pharm in abstract)"},{"metadata":{"trusted":true},"cell_type":"code","source":"non_pharm = m[(m['abstract'].str.contains('non-pharm'))]","execution_count":75,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"topic_cols = [x for x in m.columns if 'Topic ' in x]","execution_count":76,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"non_pharm_topics = non_pharm[topic_cols].idxmax(axis=1)","execution_count":77,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def most_frequent(List): \n    return max(set(List), key = List.count)","execution_count":78,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Take the topics that match most for NPI modeling papers, then find all papers with that as their top topic"},{"metadata":{"trusted":true},"cell_type":"code","source":"Counter(non_pharm_topics)","execution_count":79,"outputs":[{"output_type":"execute_result","execution_count":79,"data":{"text/plain":"Counter({'Topic 3': 50, 'Topic 1': 5, 'Topic 4': 2})"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"top_topic = most_frequent(list(non_pharm_topics))\n\ntop_topic","execution_count":80,"outputs":[{"output_type":"execute_result","execution_count":80,"data":{"text/plain":"'Topic 3'"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"m['Top_Topic'] = m[topic_cols].idxmax(axis=1)","execution_count":81,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"m.groupby('Top_Topic').size()","execution_count":82,"outputs":[{"output_type":"execute_result","execution_count":82,"data":{"text/plain":"Top_Topic\nTopic 0    10939\nTopic 1     7218\nTopic 2     7439\nTopic 3    10932\nTopic 4     5675\ndtype: int64"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"top_topic_papers = m[m['Top_Topic'] == top_topic]","execution_count":83,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Keywords\n\n- Need a core covid keyword\n- And need a topic keyword"},{"metadata":{"trusted":true},"cell_type":"code","source":"covid_keywords = ['corona', 'covid']","execution_count":84,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"intervention_keywords = ['social distancing',\n                        'contact tracing',\n                        'case isolation',\n                        'shelter-in-place',\n                        'stay-at-home',\n                        'movement restriction',\n                        'event cancel',\n                        'face mask',\n                        'facial mask',\n                        'travel ban',\n                        'school closure']","execution_count":85,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def find_papers_w_keywords(topic_keywords, papers):\n    for keyword in topic_keywords:\n        num_papers_title = len(papers[(papers['title'].str.contains(keyword)) & \n                                        (papers['title'])])\n        num_papers_abstract = len(papers[papers['abstract'].str.contains(keyword)])\n        print ('Identified {} papers with \"{}\" in title, {} relevant papers with \"{}\" in abstract'\\\n                       .format(num_papers_title, keyword, num_papers_abstract, keyword)) ","execution_count":86,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Identify core papers - about COVID-19 (keyword search and published date)"},{"metadata":{"trusted":true},"cell_type":"code","source":"date_filter = '2019-12-01'","execution_count":87,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"find_papers_w_keywords(covid_keywords, top_topic_papers)","execution_count":88,"outputs":[{"output_type":"stream","text":"Identified 512 papers with \"corona\" in title, 1407 relevant papers with \"corona\" in abstract\nIdentified 5 papers with \"covid\" in title, 37 relevant papers with \"covid\" in abstract\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"top_topic_papers['core_abstract'] = top_topic_papers['abstract'].apply(lambda x: any([k in x for k in covid_keywords]))","execution_count":89,"outputs":[{"output_type":"stream","text":"/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  \"\"\"Entry point for launching an IPython kernel.\n","name":"stderr"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"covid_papers = top_topic_papers[(top_topic_papers['core_abstract'] == True) & \n                                (top_topic_papers['publish_time'] >= date_filter)]","execution_count":90,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Intervention Papers"},{"metadata":{"trusted":true},"cell_type":"code","source":"for keyword in intervention_keywords:\n    covid_papers[keyword] = covid_papers['abstract'].str.contains(keyword)","execution_count":91,"outputs":[{"output_type":"stream","text":"/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  \n","name":"stderr"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"covid_papers['# Keywords in Abstract'] = covid_papers[intervention_keywords].sum(axis=1)","execution_count":92,"outputs":[{"output_type":"stream","text":"/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  \"\"\"Entry point for launching an IPython kernel.\n","name":"stderr"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"find_papers_w_keywords(intervention_keywords, covid_papers)","execution_count":93,"outputs":[{"output_type":"stream","text":"Identified 3 papers with \"social distancing\" in title, 36 relevant papers with \"social distancing\" in abstract\nIdentified 1 papers with \"contact tracing\" in title, 16 relevant papers with \"contact tracing\" in abstract\nIdentified 0 papers with \"case isolation\" in title, 5 relevant papers with \"case isolation\" in abstract\nIdentified 0 papers with \"shelter-in-place\" in title, 0 relevant papers with \"shelter-in-place\" in abstract\nIdentified 0 papers with \"stay-at-home\" in title, 1 relevant papers with \"stay-at-home\" in abstract\nIdentified 0 papers with \"movement restriction\" in title, 2 relevant papers with \"movement restriction\" in abstract\nIdentified 0 papers with \"event cancel\" in title, 0 relevant papers with \"event cancel\" in abstract\nIdentified 0 papers with \"face mask\" in title, 4 relevant papers with \"face mask\" in abstract\nIdentified 0 papers with \"facial mask\" in title, 2 relevant papers with \"facial mask\" in abstract\nIdentified 0 papers with \"travel ban\" in title, 9 relevant papers with \"travel ban\" in abstract\nIdentified 1 papers with \"school closure\" in title, 8 relevant papers with \"school closure\" in abstract\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"intervention_papers = covid_papers[covid_papers['# Keywords in Abstract'] > 1]","execution_count":94,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(intervention_papers)","execution_count":95,"outputs":[{"output_type":"execute_result","execution_count":95,"data":{"text/plain":"12"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"intervention_papers.to_csv(\"intervention_papers_metadata.csv\", index=False)","execution_count":96,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Search Full Papers for relevant sentences and paragraphs"},{"metadata":{"trusted":true},"cell_type":"code","source":"def find_keyword(keywords, text):\n    \"\"\"\n    Iterates through a list of keywords and searches them in a string of text.\n\n    inputs:\n      keywords: list of keywords\n      text: string of text\n\n    output: number of times keywords are found in the text\n    \"\"\"\n    find = []\n    for keyword in keywords:\n        find.extend(re.findall(keyword, text.lower()))\n    return len(find)","execution_count":97,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def search_body_text(sha, folder1, folder2, keywords, sentence_only):\n    \"\"\"\n    Searches a single full length text for sentences/paragraphs which contain a list of keywords.\n\n    inputs:\n      sha: sha file name\n      folder1: text folder name\n      folder2: pdf or pmc folder name\n      keywords: list of keywords to search for\n      sentence_only: whether or not to show sentence only or full paragraph\n    \n    output: list of sentences/paragraphs found containing keywords\n    \"\"\"\n\n    #open text file\n    with open('/kaggle/input/CORD-19-research-challenge/'+folder1+'/'+folder1+'/'+folder2+'/'+sha+'.json') as f:\n        file = json.load(f)\n    \n    found = []\n    for text_dict in file[\"body_text\"]:\n        \n        #if show_sentence_only, then split the paragraph into sentences, then look for keywords\n        if sentence_only:\n            sentences = text_dict[\"text\"].split(\". \")\n            for sentence in sentences:\n                count = find_keyword(keywords, sentence)\n                if count > 0:\n                    found.append(sentence)\n                    \n        #otherwise, show the whole paragraph\n        else:\n            count = find_keyword(keywords, text_dict[\"text\"])\n            if count > 0:\n                #print(text_dict[\"section\"])\n                found.append(text_dict[\"text\"])\n                \n    return(found)","execution_count":98,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def automated_lit_search(metadata_subset, keywords, sentence_only=True):\n    \"\"\"\n    Creates a table keyword findings.\n    \n    inputs:\n      metadata_subset: subset of metadata file to search\n      keywords: list of keywords to search\n      sentence_only: whether or not to show sentence only or full paragraph\n    \n    output: dataframe table of results with columns containing index, title, and text snippet\n    \"\"\"\n    results = []\n    \n    indices = metadata_subset[metadata_subset['has_pdf_parse'] == True].index\n    indices_pmc = metadata_subset[metadata_subset['has_pmc_xml_parse'] == True].index\n    indices.append(indices_pmc)\n    \n    for index in indices:\n        \n        #find text location\n        sha = metadata_subset[\"sha\"][index].split(';')[0]\n        folder1 = metadata_subset[\"full_text_file\"][index]\n        if metadata_subset['has_pdf_parse'][index] == True:\n            folder2 = 'pdf_json'\n        elif metadata_subset['has_pmc_xml_parse'][index] == True:\n            folder2 = 'pmc_json'\n        \n        #open text and search for keywords\n        found = search_body_text(sha, folder1, folder2, keywords, sentence_only)\n        if len(found) > 0:\n            for f in found:\n                results.append([index, metadata_subset[\"title\"][index], f])\n                \n    results_df = pd.DataFrame(results, columns=[\"index\",\"title\",\"text\"])\n    return(results_df)","execution_count":99,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"intervention_sentences = automated_lit_search(intervention_papers, intervention_keywords, True)\nintervention_sentences.to_csv('intervention_sentences.csv', index=False)","execution_count":100,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"intervention_paragraphs = automated_lit_search(intervention_papers, intervention_keywords, False)\nintervention_paragraphs.to_csv('intervention_paragraphs.csv', index=False)","execution_count":101,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"list(intervention_papers['title'])","execution_count":102,"outputs":[{"output_type":"execute_result","execution_count":102,"data":{"text/plain":"['Interventions to mitigate early spread of SARS-CoV-2 in Singapore: a modelling study',\n 'School closure and management practices during coronavirus outbreaks including COVID-19: a rapid systematic review',\n 'Impact of school closures for COVID-19 on the US health-care workforce and net mortality: a modelling study',\n 'Sentinel Event Surveillance to Estimate Total SARS-CoV-2 Infections, United States',\n 'Age profile of susceptibility, mixing, and social distancing shape the dynamics of the novel coronavirus disease 2019 outbreak in China',\n 'The Effectiveness of Social Distancing in Mitigating COVID-19 Spread: a modelling analysis',\n 'A Social Network Model of the COVID-19 Pandemic',\n 'A Genomic Survey of SARS-CoV-2 Reveals Multiple Introductions into Northern California without a Predominant Lineage',\n 'Pandemic Politics: Timing State-Level Social Distancing Responses to COVID-19',\n 'Will novel virus go pandemic or be contained?',\n 'Estimating Risk for Death from 2019 Novel Coronavirus Disease, China, January-February 2020',\n 'Feasibility of controlling COVID-19 outbreaks by isolation of cases and contacts']"},"metadata":{}}]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}